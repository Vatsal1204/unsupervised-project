{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cce98743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded user features from: mixed_dataset.csv shape: (10000, 10)\n",
      "No interactions.csv found. Generating synthetic interactions (implicit) ...\n",
      "Saved synthetic interactions.csv with 149484 rows\n",
      "Preprocessed user features shape: (10000, 19)\n",
      "Built user-item matrix: (10000, 200)\n",
      "KMeans clusters: (array([0, 1, 2, 3, 4, 5, 6, 7], dtype=int32), array([1162, 1429, 1424, 1189, 1130, 1314, 1308, 1044]))\n",
      "Saved recommendations_kmeans.csv\n",
      "Hierarchical clusters: (array([0, 1, 2, 3, 4, 5, 6, 7]), array([1785, 1771, 1265,  980,  959,  998, 1176, 1066]))\n",
      "Saved recommendations_hierarchical.csv\n",
      "DBSCAN clusters unique labels: (array([-1]), array([10000]))\n",
      "Saved recommendations_dbscan.csv\n",
      "\n",
      "Example recommendations (user_id -> top 5 items):\n",
      "KMeans user 0 -> [141, 183, 157, 6, 108]\n",
      "Hierarchical user 0 -> [105, 183, 141, 64, 147]\n",
      "DBSCAN user 0 -> [105, 54, 183, 106, 72]\n"
     ]
    }
   ],
   "source": [
    "# recommender_clustering.py\n",
    "# Simple recommender built on user clustering (KMeans, Hierarchical, DBSCAN)\n",
    "# Place this script and your \"mixed dataset.csv\" in same folder and run.\n",
    "# If no interactions file exists, synthetic interactions will be generated.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# ------------------------\n",
    "# Utilities / Robust OHE\n",
    "# ------------------------\n",
    "def make_onehot_encoder(**kwargs):\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    try:\n",
    "        return OneHotEncoder(**kwargs, sparse=False)\n",
    "    except TypeError:\n",
    "        return OneHotEncoder(**kwargs, sparse_output=False)\n",
    "\n",
    "def preprocess_user_features(df):\n",
    "    # encode categorical, scale numeric (simple, compatible)\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "\n",
    "    numerical_cols = df.select_dtypes(exclude='object').columns.tolist()\n",
    "    categorical_cols = df.select_dtypes(include='object').columns.tolist()\n",
    "\n",
    "    ohe = make_onehot_encoder(drop='first', handle_unknown='ignore')\n",
    "    scaler = StandardScaler()\n",
    "    ct = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('ohe', ohe, categorical_cols),\n",
    "            ('scale', scaler, numerical_cols)\n",
    "        ],\n",
    "        remainder='drop'\n",
    "    )\n",
    "    X = ct.fit_transform(df)\n",
    "    # build names (optional)\n",
    "    names = []\n",
    "    if categorical_cols:\n",
    "        try:\n",
    "            names = list(ct.named_transformers_['ohe'].get_feature_names_out(categorical_cols))\n",
    "        except Exception:\n",
    "            cats = ct.named_transformers_['ohe'].categories_\n",
    "            for col, cats_vals in zip(categorical_cols, cats):\n",
    "                for c in cats_vals[1:]:\n",
    "                    names.append(f\"{col}_{c}\")\n",
    "    names = names + numerical_cols\n",
    "    X = np.asarray(X)\n",
    "    return X, names, ct\n",
    "\n",
    "# ------------------------\n",
    "# Recommendation helpers\n",
    "# ------------------------\n",
    "def generate_synthetic_interactions(n_users, n_items=100, min_items=5, max_items=20, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    rows = []\n",
    "    for u in range(n_users):\n",
    "        k = rng.integers(min_items, max_items + 1)\n",
    "        items = rng.choice(n_items, size=k, replace=False)\n",
    "        for it in items:\n",
    "            # rating or purchase count â€” use implicit 1\n",
    "            rows.append({'user_id': u, 'item_id': int(it), 'rating': 1})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def build_user_item_matrix(interactions, user_index_map, n_items):\n",
    "    # implicit feedback matrix (users x items) with counts\n",
    "    n_users = len(user_index_map)\n",
    "    mat = np.zeros((n_users, n_items), dtype=int)\n",
    "    for _, row in interactions.iterrows():\n",
    "        uid = row['user_id']\n",
    "        iid = int(row['item_id'])\n",
    "        if uid in user_index_map:\n",
    "            uidx = user_index_map[uid]\n",
    "            if 0 <= iid < n_items:\n",
    "                mat[uidx, iid] += int(row.get('rating', 1))\n",
    "    return mat\n",
    "\n",
    "def recommend_topN_by_cluster(user_idx, cluster_labels, user_item_matrix, N=5):\n",
    "    \"\"\"\n",
    "    For a user index (0-based), find users in same cluster,\n",
    "    sum their item counts, and recommend top N items not already consumed by the user.\n",
    "    \"\"\"\n",
    "    label = cluster_labels[user_idx]\n",
    "    if label == -1:\n",
    "        # noise: recommend global popular\n",
    "        pop = user_item_matrix.sum(axis=0)\n",
    "    else:\n",
    "        members = np.where(cluster_labels == label)[0]\n",
    "        if len(members) == 0:\n",
    "            pop = user_item_matrix.sum(axis=0)\n",
    "        else:\n",
    "            pop = user_item_matrix[members].sum(axis=0)\n",
    "    # items the user already consumed\n",
    "    consumed = np.where(user_item_matrix[user_idx] > 0)[0]\n",
    "    # rank items by popularity\n",
    "    ranked = np.argsort(-pop)\n",
    "    # filter out consumed\n",
    "    recs = [int(i) for i in ranked if i not in set(consumed)]\n",
    "    return recs[:N]\n",
    "\n",
    "# ------------------------\n",
    "# Main flow\n",
    "# ------------------------\n",
    "def main():\n",
    "    # 1) Load users (features)\n",
    "    path = Path.cwd()\n",
    "    # find CSV file that looks like mixed dataset\n",
    "    possible = [f for f in path.glob(\"*.csv\") if \"mixed\" in f.name.lower() and \"dataset\" in f.name.lower()]\n",
    "    if possible:\n",
    "        user_file = possible[0].name\n",
    "    else:\n",
    "        # fallback exact name\n",
    "        if (path / \"mixed dataset.csv\").exists():\n",
    "            user_file = \"mixed dataset.csv\"\n",
    "        elif (path / \"mixed_dataset.csv\").exists():\n",
    "            user_file = \"mixed_dataset.csv\"\n",
    "        else:\n",
    "            raise FileNotFoundError(\"Could not find your mixed dataset CSV. Put it in this folder and include 'mixed' and 'dataset' in the filename.\")\n",
    "    users = pd.read_csv(user_file)\n",
    "    print(\"Loaded user features from:\", user_file, \"shape:\", users.shape)\n",
    "\n",
    "    # assign user ids if none present -- use index as user_id\n",
    "    users = users.reset_index(drop=True)\n",
    "    users['user_id'] = users.index  # ensure a user_id column for mapping\n",
    "\n",
    "    # 2) Load or generate interactions.csv (user_id, item_id, rating)\n",
    "    if (path / \"interactions.csv\").exists():\n",
    "        interactions = pd.read_csv(\"interactions.csv\")\n",
    "        print(\"Loaded interactions.csv with\", len(interactions), \"rows\")\n",
    "    else:\n",
    "        print(\"No interactions.csv found. Generating synthetic interactions (implicit) ...\")\n",
    "        interactions = generate_synthetic_interactions(n_users=len(users), n_items=200, min_items=5, max_items=25)\n",
    "        # user ids in synthetic are 0..n-1, match our user_id\n",
    "        interactions.to_csv(\"interactions.csv\", index=False)\n",
    "        print(\"Saved synthetic interactions.csv with\", len(interactions), \"rows\")\n",
    "\n",
    "    # 3) Preprocess user features for clustering\n",
    "    feature_df = users.drop(columns=['user_id'], errors='ignore')\n",
    "    X, names, _ = preprocess_user_features(feature_df)\n",
    "    print(\"Preprocessed user features shape:\", X.shape)\n",
    "\n",
    "    # 4) Build user-item matrix\n",
    "    n_items = int(interactions['item_id'].max()) + 1\n",
    "    user_index_map = {uid: idx for idx, uid in enumerate(users['user_id'].tolist())}\n",
    "    uim = build_user_item_matrix(interactions, user_index_map, n_items)\n",
    "    print(\"Built user-item matrix:\", uim.shape)\n",
    "\n",
    "    # Clustering & recommending functions\n",
    "    from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "    from sklearn.cluster import DBSCAN\n",
    "\n",
    "    # ------ KMeans -------\n",
    "    k = 8\n",
    "    km = KMeans(n_clusters=k, init='k-means++', random_state=42, n_init=10)\n",
    "    km_labels = km.fit_predict(X)\n",
    "    print(\"KMeans clusters:\", np.unique(km_labels, return_counts=True))\n",
    "    # recommendations per user\n",
    "    recs_k = []\n",
    "    for uid in users['user_id']:\n",
    "        idx = user_index_map[uid]\n",
    "        recs = recommend_topN_by_cluster(idx, km_labels, uim, N=10)\n",
    "        recs_k.append({'user_id': int(uid), 'recommendations': recs})\n",
    "    pd.DataFrame(recs_k).to_csv(\"recommendations_kmeans.csv\", index=False)\n",
    "    print(\"Saved recommendations_kmeans.csv\")\n",
    "\n",
    "    # ------ Hierarchical -------\n",
    "    H = 8\n",
    "    agg = AgglomerativeClustering(n_clusters=H, linkage='ward')\n",
    "    agg_labels = agg.fit_predict(X)\n",
    "    print(\"Hierarchical clusters:\", np.unique(agg_labels, return_counts=True))\n",
    "    recs_h = []\n",
    "    for uid in users['user_id']:\n",
    "        idx = user_index_map[uid]\n",
    "        recs = recommend_topN_by_cluster(idx, agg_labels, uim, N=10)\n",
    "        recs_h.append({'user_id': int(uid), 'recommendations': recs})\n",
    "    pd.DataFrame(recs_h).to_csv(\"recommendations_hierarchical.csv\", index=False)\n",
    "    print(\"Saved recommendations_hierarchical.csv\")\n",
    "\n",
    "    # ------ DBSCAN -------\n",
    "    # choose eps heuristically: use median of k-distances approach simplified by scaling\n",
    "    # We'll try a small eps and fallback if all noise.\n",
    "    db = DBSCAN(eps=1.5, min_samples=X.shape[1]+1)\n",
    "    db_labels = db.fit_predict(X)\n",
    "    # if DBSCAN produced all -1 (unlikely), try a smaller eps\n",
    "    if np.all(db_labels == -1):\n",
    "        db = DBSCAN(eps=0.8, min_samples=5)\n",
    "        db_labels = db.fit_predict(X)\n",
    "    print(\"DBSCAN clusters unique labels:\", np.unique(db_labels, return_counts=True))\n",
    "    recs_d = []\n",
    "    for uid in users['user_id']:\n",
    "        idx = user_index_map[uid]\n",
    "        recs = recommend_topN_by_cluster(idx, db_labels, uim, N=10)\n",
    "        recs_d.append({'user_id': int(uid), 'recommendations': recs})\n",
    "    pd.DataFrame(recs_d).to_csv(\"recommendations_dbscan.csv\", index=False)\n",
    "    print(\"Saved recommendations_dbscan.csv\")\n",
    "\n",
    "    # Print examples\n",
    "    print(\"\\nExample recommendations (user_id -> top 5 items):\")\n",
    "    for method, df_rec in [(\"KMeans\", recs_k), (\"Hierarchical\", recs_h), (\"DBSCAN\", recs_d)]:\n",
    "        sample = df_rec[0]  # first user\n",
    "        print(method, \"user\", sample['user_id'], \"->\", sample['recommendations'][:5])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
